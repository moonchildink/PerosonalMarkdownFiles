# Skeleton Aware Multi-modal Sign Language Recognition

## Abstract

手语被聋哑人或者语言障碍者广泛应用来进行交流，但这项技能通常难以掌握。手语识别(Sign Language Recognition,SLR)目的在于，通过识别给定视频来弥补手语使用者与他人沟通的困难。这是一项非常重要的工作，但是颇具挑战——手语通过手部高速、复杂的运动，身体姿势甚至面部表情来进行表达。最近，基于骨架（Skeleton）的动作识别，吸引了越来越多的注意力，因为它能够实现主体与背景之间很好的分割。但是，基于骨骼的手语识别仍然处于探索之中，因为我们还缺少手部关键的的注释信息（annotations）。一些研究者，使用伴有姿态预测的手部探测（hand detectors）来提取关键点，并且通过神经网络来实现识别，但是没有良好的基于RGB视频数据的处理方法。因此，我们提出了一种新的识别结构：*Skeleton Aware Multi-modal SLR framework(SAM-SLR)*来采集多模态信息（Multi-modal），来获取更高的准确率。特别的，我们提出了*Sign Language Graph Convolution Network*,对嵌入动力学和一种全新的可分离的卷积时空网络进行建模来利用骨骼特征。RGB和深度数据同样纳入并且组装到了我们的框架之中，来提供全局信息，这是对基于骨骼方法的*SL-GCN*和*SSTCN*的补充。结果，在*2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge*z中，*SAM-SLR*在RGB数据($98.42\%$)和RGB-D数据($98.53\%$)上同时实现了最好的效果。Our code is available at [jackyjsy/CVPR21Chal-SLR: This repo contains the official code of our work SAM-SLR which won the CVPR 2021 Challenge on Large Scale Signer Independent Isolated Sign Language Recognition. (github.com)](https://github.com/jackyjsy/CVPR21Chal-SLR).



## 1. Introduction

手语是一中基于手部动作、身体姿势以及面部表情的动态语言。这是一种聋哑人与语言障碍患者的有效表达方式。理解并利用手语需要大量的时间和训练，这对于公众显然是无法实现的。而且，手语同样受语言和文化影响，这更加限制了手语的流行。过去十年中，机器学习和计算机视觉获得了长足的发展，重要的是探索手语识别技术来自动翻译手语，并且帮助聋哑人在日常生活中与他人流畅交流。

相比于常规动作识别（Conventional action recognition），手语识别更具挑战性。**首先，**手语需要全局身体信息和精确的肢体动作来清晰而准确的表达信息。面部表情同样可以用来表达情绪。相似的动作可以表达不同的信息。**其次，**不同的表演者表现了不同的手语。要实现收集更多的样本相当困难。

传统的手语识别方法基本上基于handcrafted features，比如HOG和SIFT，使用传统的分类方法，比如kNN和SVM。随着深度学习实现了明显的进步，常规的时序学习方法（RNN和LSTM）和动作识别框架（3D CNNs） 最初被用来实现手语识别。为了更加有效的捕捉local motion information，注意力机制结合了其他方法来实现了更高的准确率。另外，也有一些方法使用了semantic detection/segmentation models to explicitly guide the recognition network in a two-stage pipeline.在两阶段流水线中显式地引导识别网络。

recently,在动作识别任务中，skeleton based methods变得更加流行，也吸引了更多的注意力因为他们在动态环境与复杂背景中有更强的鲁棒性。因为基于骨架的方法为RGB模态提供了补充信息，提升了全局的著鳄雀形。然而，一些缺陷阻碍了将他们拓展到SLR任务之中。这些skeleton-based action recognition methods依赖动作捕捉系统提供的真实skeleton信息，将其限制于可控环境下获取的少量数据集之中。另外，很多动作捕捉系统仅仅考虑身体、手臂信息而忽略了手部的详细信息。这些骨骼数据包含的信息不足以完成手语识别。而通过RNN-based model识别手语，其获取的信息不可信，并且以RNN为基础的model，不能很好的动态获取人类骨骼信息。

受最近的身体动作预测的发展的启发，在本文中，我们提出了一种全新的skeleton-based手语识别方法，使用预训练的whole-body动作预测器来获取全身关键点和特征。我们为SLR任务设计了一种全新的spatio-temporal skeleton graph，并提出了一种手语图卷积神经网络来模拟动态嵌入。为了利用全身关键点信息，我们提出了一种separable spatial temporal convolution network(可分离时空卷积网络)来提取全身骨骼信息。对于动态识别的研究已经表明多模态数据相辅相成，并提供了额外的识别信息。为了进一步提升识别准确率，我们提出了*Skeleton Aware Multi-modal SLR framework*来集成我们提出的skeleton-based methods和其他模态在RGB和RGB-Depth数据中。我们的主要贡献可以总结如下：

-   我们专门为SLR任务设计了全新的骨骼图，使用预训练的全身姿态预测器，提取全身关键点数据和并进行图降维，不需要额外的标注信息。
-   我们提出了**SL-GCN**来从全身骨骼数据中提取信息。据我们所知，这是第一次使用全身骨骼信息来实现SLR任务的成功尝试。
-   我们提出了全新的**SSTCN**来进一步利用全身骨骼特征，这可以显著提升全身关键点信息提取的准确率，相较于传统的3D卷积神经网络。
-   我们提出了**SAM-SLR**框架。该框架可以从6中模态信息中学习，并且实现了state-of-the-art performance在AUSTL数据集上。我们提出的方法在**CVPR-21 Challenge on isolated SLR**提供的两种数据上都获得了第一名。





## 2. Related work

**Sign Language Recognition (SLR) **